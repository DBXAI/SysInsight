{
  "param_name": "random_page_cost",
  "function_name": "initial_cost_hashjoin",
  "analysis": "1. Parameters influence database performance by controlling key functions:\n   - `random_page_cost` affects `initial_cost_hashjoin` by estimating the cost of disk access, which in turn influences query plan selection.\n   - Mechanism: A higher `random_page_cost` value implies a higher cost for random disk access. In the initial cost calculation, if `inner_path` or `outer_path` requires random disk data access, `initial_cost_hashjoin` will consider this high cost in its estimation. Consequently, if there is significant disk data access, it may prioritize other query plans that are less dependent on random disk access.\n   - Impact on database performance: This may lead to query plan selection favoring sequential scans or reducing random disk I/O, thereby affecting execution time. A lower `random_page_cost` might result in the selection of plans that are not efficient.\n\n2. Based on the execution status of `initial_cost_hashjoin` and related function segments, provide optimization suggestions for `random_page_cost`:\n   - If other functions are involved, it is necessary to monitor other cost calculation functions that influence join strategy selection, such as `final_cost_hashjoin` or other functions that might have an impact on I/O costs.\n   - According to the flame graph sampling rate of `initial_cost_hashjoin` and other functions, it is recommended to adjust `random_page_cost` by analyzing the frequency of I/O operations. When the flame graph shows that I/O operation time is excessively high and disk access bottlenecks repeatedly occur, it is suggested to lower `random_page_cost` to encourage plan selection that makes more efficient use of memory. Conversely, it is recommended to increase it to prevent unreasonable memory cost usage. The basis for this is to confirm through sampling results whether it is a disk bottleneck or excessive memory usage causing performance degradation.",
  "code_snippets": "initial_cost_hashjoin(PlannerInfo *root, JoinCostWorkspace *workspace,\n\t\t\t\t\t  JoinType jointype,\n\t\t\t\t\t  List *hashclauses,\n\t\t\t\t\t  Path *outer_path, Path *inner_path,\n\t\t\t\t\t  JoinPathExtraData *extra,\n\t\t\t\t\t  bool parallel_hash)\n{\n\tCost\t\tstartup_cost = 0;\n\tCost\t\trun_cost = 0;\n\tdouble\t\touter_path_rows = outer_path->rows;\n\tdouble\t\tinner_path_rows = inner_path->rows;\n\tdouble\t\tinner_path_rows_total = inner_path_rows;\n\tint\t\t\tnum_hashclauses = list_length(hashclauses);\n\tint\t\t\tnumbuckets;\n\tint\t\t\tnumbatches;\n\tint\t\t\tnum_skew_mcvs;\n\tsize_t\t\tspace_allowed;\t/* unused */\n\n\t/* cost of source data */\n\tstartup_cost += outer_path->startup_cost;\n\trun_cost += outer_path->total_cost - outer_path->startup_cost;\n\tstartup_cost += inner_path->total_cost;\n\n\t/*\n\t * Cost of computing hash function: must do it once per input tuple. We\n\t * charge one cpu_operator_cost for each column's hash function.  Also,\n\t * tack on one cpu_tuple_cost per inner row, to model the costs of\n\t * inserting the row into the hashtable.\n\t *\n\t * XXX when a hashclause is more complex than a single operator, we really\n\t * should charge the extra eval costs of the left or right side, as\n\t * appropriate, here.  This seems more work than it's worth at the moment.\n\t */\n\tstartup_cost += (cpu_operator_cost * num_hashclauses + cpu_tuple_cost)\n\t\t* inner_path_rows;\n\trun_cost += cpu_operator_cost * num_hashclauses * outer_path_rows;\n\n\t/*\n\t * If this is a parallel hash build, then the value we have for\n\t * inner_rows_total currently refers only to the rows returned by each\n\t * participant.  For shared hash table size estimation, we need the total\n\t * number, so we need to undo the division.\n\t */\n\tif (parallel_hash)\n\t\tinner_path_rows_total *= get_parallel_divisor(inner_path);\n\n\t/*\n\t * Get hash table size that executor would use for inner relation.\n\t *\n\t * XXX for the moment, always assume that skew optimization will be\n\t * performed.  As long as SKEW_HASH_MEM_PERCENT is small, it's not worth\n\t * trying to determine that for sure.\n\t *\n\t * XXX at some point it might be interesting to try to account for skew\n\t * optimization in the cost estimate, but for now, we don't.\n\t */\n\tExecChooseHashTableSize(inner_path_rows_total,\n\t\t\t\t\t\t\tinner_path->pathtarget->width,\n\t\t\t\t\t\t\ttrue,\t/* useskew */\n\t\t\t\t\t\t\tparallel_hash,\t/* try_combined_hash_mem */\n\t\t\t\t\t\t\touter_path->parallel_workers,\n\t\t\t\t\t\t\t&space_allowed,\n\t\t\t\t\t\t\t&numbuckets,\n\t\t\t\t\t\t\t&numbatches,\n\t\t\t\t\t\t\t&num_skew_mcvs);\n\n\t/*\n\t * If inner relation is too big then we will need to \"batch\" the join,\n\t * which implies writing and reading most of the tuples to disk an extra\n\t * time.  Charge seq_page_cost per page, since the I/O should be nice and\n\t * sequential.  Writing the inner rel counts as startup cost, all the rest\n\t * as run cost.\n\t */\n\tif (numbatches > 1)\n\t{\n\t\tdouble\t\touterpages = page_size(outer_path_rows,\n\t\t\t\t\t\t\t\t\t\t   outer_path->pathtarget->width);\n\t\tdouble\t\tinnerpages = page_size(inner_path_rows,\n\t\t\t\t\t\t\t\t\t\t   inner_path->pathtarget->width);\n\n\t\tstartup_cost += seq_page_cost * innerpages;\n\t\trun_cost += seq_page_cost * (innerpages + 2 * outerpages);\n\t}\n\n\t/* CPU costs left for later */\n\n\t/* Public result fields */\n\tworkspace->startup_cost = startup_cost;\n\tworkspace->total_cost = startup_cost + run_cost;\n\t/* Save private data for final_cost_hashjoin */\n\tworkspace->run_cost = run_cost;\n\tworkspace->numbuckets = numbuckets;\n\tworkspace->numbatches = numbatches;\n\tworkspace->inner_rows_total = inner_path_rows_total;\n}",
  "timestamp": "2025-10-30T15:57:44.415187"
}