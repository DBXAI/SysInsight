{
  "param_name": "innodb_buffer_pool_instances",
  "function_name": "buf_read_ahead_linear",
  "analysis": "1. Parameters control the impact of key functions on database performance:\n    - `innodb_buffer_pool_instances` affects `buf_read_ahead_linear` by dividing the buffer pool into multiple instances to reduce lock contention, thereby impacting database performance related to linear read-ahead efficiency.\n    - Mechanism: `innodb_buffer_pool_instances` provides multiple instances for the buffer pool, which can reduce lock contention issues caused by multiple threads concurrently accessing the buffer pool. Each instance has its own locks and mutexes, meaning multiple threads can concurrently access multiple buffer pool instances without contending for lock resources. This directly affects the execution efficiency of `buf_read_ahead_linear`, especially in high concurrency scenarios, potentially increasing read-ahead opportunities.\n    - Database performance impact: Increasing the number of buffer pool instances with `innodb_buffer_pool_instances` can improve the throughput and response time of the buffer pool in high concurrency environments, as less lock contention allows the `buf_read_ahead_linear` function to perform read-ahead operations more efficiently, thereby accessing data pages more quickly.\n\n2. Based on the execution status of `buf_read_ahead_linear` and related function segments, provide optimization suggestions for `innodb_buffer_pool_instances`:\n    - From flame graph analysis, it is also necessary to monitor `buf_pool_get`, `buf_page_hash_get_s_locked`, and `buf_read_page_low`, as these functions are involved in buffer pool page retrieval and read operations.\n    - If the flame graph sampling rate of `buf_read_ahead_linear` and `buf_pool_get`, `buf_page_hash_get_s_locked`, `buf_read_page_low` shows frequent lock contention patterns in high concurrency environments, it is recommended to increase the number of `innodb_buffer_pool_instances` to distribute lock contention and improve access efficiency. The rationale is that increasing `innodb_buffer_pool_instances` can effectively reduce contention and improve the throughput of the buffer pool during concurrent access. However, too many instances may increase overhead, and a reasonable high concurrency scheduling setting should be determined based on specific sampling analysis.",
  "code_snippets": "ulint buf_read_ahead_linear(const page_id_t &page_id,\n                            const page_size_t &page_size, bool inside_ibuf) {\n  buf_pool_t *buf_pool = buf_pool_get(page_id);\n  buf_page_t *bpage;\n  buf_frame_t *frame;\n  buf_page_t *pred_bpage = nullptr;\n  std::chrono::steady_clock::time_point pred_bpage_is_accessed;\n  page_no_t pred_offset;\n  page_no_t succ_offset;\n  int asc_or_desc;\n  page_no_t new_offset;\n  ulint fail_count;\n  page_no_t low, high;\n  dberr_t err;\n  page_no_t i;\n  const page_no_t buf_read_ahead_linear_area = buf_pool->read_ahead_area;\n  page_no_t threshold;\n\n  /* check if readahead is disabled */\n  if (!srv_read_ahead_threshold) {\n    return (0);\n  }\n\n  if (srv_startup_is_before_trx_rollback_phase) {\n    /* No read-ahead to avoid thread deadlocks */\n    return (0);\n  }\n\n  low = (page_id.page_no() / buf_read_ahead_linear_area) *\n        buf_read_ahead_linear_area;\n  high = (page_id.page_no() / buf_read_ahead_linear_area + 1) *\n         buf_read_ahead_linear_area;\n\n  if ((page_id.page_no() != low) && (page_id.page_no() != high - 1)) {\n    /* This is not a border page of the area: return */\n\n    return (0);\n  }\n\n  if (ibuf_bitmap_page(page_id, page_size) || trx_sys_hdr_page(page_id)) {\n    /* If it is an ibuf bitmap page or trx sys hdr, we do\n    no read-ahead, as that could break the ibuf page access\n    order */\n\n    return (0);\n  }\n\n  /* Remember the tablespace version before we ask the tablespace size\n  below: if DISCARD + IMPORT changes the actual .ibd file meanwhile, we\n  do not try to read outside the bounds of the tablespace! */\n  ulint space_size;\n\n  if (fil_space_t *space = fil_space_acquire_silent(page_id.space())) {\n    space_size = space->size;\n\n    fil_space_release(space);\n\n    if (high > space_size) {\n      /* The area is not whole */\n      return (0);\n    }\n  } else {\n    return (0);\n  }\n\n  /* Read memory barrier */\n\n  os_rmb;\n\n  if (buf_pool->n_pend_reads >\n      buf_pool->curr_size / BUF_READ_AHEAD_PEND_LIMIT) {\n    return (0);\n  }\n\n  /* Check that almost all pages in the area have been accessed; if\n  offset == low, the accesses must be in a descending order, otherwise,\n  in an ascending order. */\n\n  asc_or_desc = 1;\n\n  if (page_id.page_no() == low) {\n    asc_or_desc = -1;\n  }\n\n  /* How many out of order accessed pages can we ignore\n  when working out the access pattern for linear readahead */\n  threshold = std::min(static_cast<page_no_t>(64 - srv_read_ahead_threshold),\n                       buf_pool->read_ahead_area);\n\n  fail_count = 0;\n\n  rw_lock_t *hash_lock;\n\n  for (i = low; i < high; i++) {\n    bpage = buf_page_hash_get_s_locked(buf_pool, page_id_t(page_id.space(), i),\n                                       &hash_lock);\n\n    if (bpage == nullptr || buf_page_is_accessed(bpage) ==\n                                std::chrono::steady_clock::time_point{}) {\n      /* Not accessed */\n      fail_count++;\n\n    } else if (pred_bpage) {\n      /* Note that buf_page_is_accessed() returns\n      the time of the first access.  If some blocks\n      of the extent existed in the buffer pool at\n      the time of a linear access pattern, the first\n      access times may be nonmonotonic, even though\n      the latest access times were linear.  The\n      threshold (srv_read_ahead_factor) should help\n      a little against this. */\n      int res = 0;\n      if (buf_page_is_accessed(bpage) == pred_bpage_is_accessed) {\n        res = 0;\n      } else if (buf_page_is_accessed(bpage) < pred_bpage_is_accessed) {\n        res = -1;\n      } else {\n        res = 1;\n      }\n      /* Accesses not in the right order */\n      if (res != 0 && res != asc_or_desc) {\n        fail_count++;\n      }\n    }\n\n    if (fail_count > threshold) {\n      /* Too many failures: return */\n      if (bpage) {\n        rw_lock_s_unlock(hash_lock);\n      }\n      return (0);\n    }\n\n    if (bpage) {\n      if (buf_page_is_accessed(bpage) !=\n          std::chrono::steady_clock::time_point{}) {\n        pred_bpage = bpage;\n        pred_bpage_is_accessed = buf_page_is_accessed(bpage);\n      }\n\n      rw_lock_s_unlock(hash_lock);\n    }\n  }\n\n  /* If we got this far, we know that enough pages in the area have\n  been accessed in the right order: linear read-ahead can be sensible */\n\n  bpage = buf_page_hash_get_s_locked(buf_pool, page_id, &hash_lock);\n\n  if (bpage == nullptr) {\n    return (0);\n  }\n\n  switch (buf_page_get_state(bpage)) {\n    case BUF_BLOCK_ZIP_PAGE:\n      frame = bpage->zip.data;\n      break;\n    case BUF_BLOCK_FILE_PAGE:\n      frame = ((buf_block_t *)bpage)->frame;\n      break;\n    default:\n      ut_error;\n      break;\n  }\n\n  /* Read the natural predecessor and successor page addresses from\n  the page; NOTE that because the calling thread may have an x-latch\n  on the page, we do not acquire an s-latch on the page, this is to\n  prevent deadlocks. Even if we read values which are nonsense, the\n  algorithm will work. */\n\n  pred_offset = fil_page_get_prev(frame);\n  succ_offset = fil_page_get_next(frame);\n\n  rw_lock_s_unlock(hash_lock);\n\n  if ((page_id.page_no() == low) && (succ_offset == page_id.page_no() + 1)) {\n    /* This is ok, we can continue */\n    new_offset = pred_offset;\n\n  } else if ((page_id.page_no() == high - 1) &&\n             (pred_offset == page_id.page_no() - 1)) {\n    /* This is ok, we can continue */\n    new_offset = succ_offset;\n  } else {\n    /* Successor or predecessor not in the right order */\n\n    return (0);\n  }\n\n  low = (new_offset / buf_read_ahead_linear_area) * buf_read_ahead_linear_area;\n  high = (new_offset / buf_read_ahead_linear_area + 1) *\n         buf_read_ahead_linear_area;\n\n  if ((new_offset != low) && (new_offset != high - 1)) {\n    /* This is not a border page of the area: return */\n\n    return (0);\n  }\n\n  if (high > space_size) {\n    /* The area is not whole, return */\n\n    return (0);\n  }\n\n  ulint count = 0;\n\n  /* If we got this far, read-ahead can be sensible: do it */\n\n  ulint ibuf_mode;\n\n  ibuf_mode = inside_ibuf ? BUF_READ_IBUF_PAGES_ONLY : BUF_READ_ANY_PAGE;\n\n  /* Since Windows XP seems to schedule the i/o handler thread\n  very eagerly, and consequently it does not wait for the\n  full read batch to be posted, we use special heuristics here */\n\n  os_aio_simulated_put_read_threads_to_sleep();\n\n  for (i = low; i < high; i++) {\n    /* It is only sensible to do read-ahead in the non-sync\n    aio mode: hence false as the first parameter */\n\n    const page_id_t cur_page_id(page_id.space(), i);\n\n    if (!ibuf_bitmap_page(cur_page_id, page_size)) {\n      count += buf_read_page_low(&err, false, IORequest::DO_NOT_WAKE, ibuf_mode,\n                                 cur_page_id, page_size, false);\n\n      if (err == DB_TABLESPACE_DELETED) {\n        ib::warn(ER_IB_MSG_142) << \"linear readahead trying to\"\n                                   \" access page \"\n                                << page_id_t(page_id.space(), i)\n                                << \" in nonexisting or being-dropped\"\n                                   \" tablespace\";\n      }\n    }\n  }\n\n  /* In simulated aio we wake the aio handler threads only after\n  queuing all aio requests. */\n\n  os_aio_simulated_wake_handler_threads();\n\n  if (count) {\n    DBUG_PRINT(\"ib_buf\", (\"linear read-ahead %lu pages, \" UINT32PF \":\" UINT32PF,\n                          count, page_id.space(), page_id.page_no()));\n  }\n\n  /* Read ahead is considered one I/O operation for the purpose of\n  LRU policy decision. */\n  buf_LRU_stat_inc_io();\n\n  buf_pool->stat.n_ra_pages_read += count;\n  return (count);\n}",
  "timestamp": "2025-10-30T15:56:31.805084"
}